	.text
	.file	"gsm_explode.c"
	.globl	gsm_explode             # -- Begin function gsm_explode
	.p2align	4, 0x90
	.type	gsm_explode,@function
gsm_explode:                            # @gsm_explode
	.cfi_startproc
# %bb.0:                                # %"0"
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset %rbp, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register %rbp
	movq	%rdi, -32(%rbp)
	movq	%rsi, -8(%rbp)
	movq	%rdx, -16(%rbp)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$15, %eax
	cmpl	$13, %eax
	je	.LBB0_2
# %bb.1:                                # %"1"
	movq	__profc_gsm_explode, %rax
	addq	$1, %rax
	movq	%rax, __profc_gsm_explode
	movl	$-1, -20(%rbp)
	jmp	.LBB0_3
.LBB0_2:                                # %"2"
	movq	__profc_gsm_explode+8, %rax
	addq	$1, %rax
	movq	%rax, __profc_gsm_explode+8
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$15, %eax
	shll	$2, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, (%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movq	-16(%rbp), %rcx
	movswl	(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, (%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$63, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 2(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$31, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 4(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$7, %eax
	shll	$2, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 6(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movq	-16(%rbp), %rcx
	movswl	6(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, 6(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$2, %eax
	andl	$15, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 8(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$3, %eax
	shll	$2, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 10(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movq	-16(%rbp), %rcx
	movswl	10(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, 10(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 12(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 14(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$127, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 16(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$1, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 18(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	-16(%rbp), %rcx
	movswl	18(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, 18(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$3, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 20(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$31, %eax
	shll	$1, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 22(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	-16(%rbp), %rcx
	movswl	22(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, 22(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 24(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 26(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 28(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movq	-16(%rbp), %rcx
	movswl	28(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, 28(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 30(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 32(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 34(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$2, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 36(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$3, %eax
	shll	$1, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 38(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	-16(%rbp), %rcx
	movswl	38(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, 38(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 40(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 42(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 44(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movq	-16(%rbp), %rcx
	movswl	44(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, 44(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 46(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 48(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$127, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 50(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$1, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 52(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	-16(%rbp), %rcx
	movswl	52(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, 52(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$3, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 54(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$31, %eax
	shll	$1, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 56(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	-16(%rbp), %rcx
	movswl	56(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, 56(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 58(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 60(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 62(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movq	-16(%rbp), %rcx
	movswl	62(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, 62(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 64(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 66(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 68(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$2, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 70(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$3, %eax
	shll	$1, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 72(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	-16(%rbp), %rcx
	movswl	72(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, 72(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 74(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 76(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 78(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movq	-16(%rbp), %rcx
	movswl	78(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, 78(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 80(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 82(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$127, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 84(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$1, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 86(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	-16(%rbp), %rcx
	movswl	86(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, 86(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$3, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 88(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$31, %eax
	shll	$1, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 90(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	-16(%rbp), %rcx
	movswl	90(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, 90(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 92(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 94(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 96(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movq	-16(%rbp), %rcx
	movswl	96(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, 96(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 98(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 100(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 102(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$2, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 104(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$3, %eax
	shll	$1, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 106(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	-16(%rbp), %rcx
	movswl	106(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, 106(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 108(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 110(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 112(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movq	-16(%rbp), %rcx
	movswl	112(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, 112(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 114(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 116(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$127, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 118(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$1, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 120(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	-16(%rbp), %rcx
	movswl	120(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, 120(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$3, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 122(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$31, %eax
	shll	$1, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 124(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	-16(%rbp), %rcx
	movswl	124(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, 124(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 126(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 128(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 130(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movq	-16(%rbp), %rcx
	movswl	130(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, 130(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 132(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 134(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 136(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$2, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 138(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$3, %eax
	shll	$1, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 140(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	-16(%rbp), %rcx
	movswl	140(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, 140(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 142(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 144(%rcx)
	movq	-8(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -8(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 146(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movq	-16(%rbp), %rcx
	movswl	146(%rcx), %edx
	orl	%eax, %edx
	movw	%dx, 146(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 148(%rcx)
	movq	-8(%rbp), %rax
	movzbl	(%rax), %eax
	andl	$7, %eax
	movq	-16(%rbp), %rcx
	movw	%ax, 150(%rcx)
	movl	$0, -20(%rbp)
.LBB0_3:                                # %"3"
	movl	-20(%rbp), %eax
	popq	%rbp
	.cfi_def_cfa %rsp, 8
	retq
.Lfunc_end0:
	.size	gsm_explode, .Lfunc_end0-gsm_explode
	.cfi_endproc
                                        # -- End function
	.hidden	__profc_gsm_explode
	.ident	"clang version 10.0.0 "
	.section	".note.GNU-stack","",@progbits
