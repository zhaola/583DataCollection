	.text
	.file	"gsm_print.c"
	.globl	gsm_print               # -- Begin function gsm_print
	.p2align	4, 0x90
	.type	gsm_print,@function
gsm_print:                              # @gsm_print
	.cfi_startproc
# %bb.0:                                # %"0"
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset %rbp, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$280, %rsp              # imm = 0x118
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	movq	%rdi, -88(%rbp)
	movq	%rsi, -240(%rbp)
	movq	%rdx, -48(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$15, %eax
	cmpl	$13, %eax
	je	.LBB0_2
# %bb.1:                                # %"1"
	movq	__profc_gsm_print, %rax
	addq	$1, %rax
	movq	%rax, __profc_gsm_print
	movl	$-1, -212(%rbp)
	jmp	.LBB0_3
.LBB0_2:                                # %"2"
	movq	__profc_gsm_print+8, %rax
	addq	$1, %rax
	movq	%rax, __profc_gsm_print+8
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$15, %eax
	shll	$2, %eax
	movw	%ax, -80(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movswl	-80(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -80(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$63, %eax
	movw	%ax, -78(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$31, %eax
	movw	%ax, -76(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$7, %eax
	shll	$2, %eax
	movw	%ax, -74(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movswl	-74(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -74(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$2, %eax
	andl	$15, %eax
	movw	%ax, -72(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$3, %eax
	shll	$2, %eax
	movw	%ax, -70(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movswl	-70(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -70(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movw	%ax, -68(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movw	%ax, -66(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$127, %eax
	movw	%ax, -208(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$1, %eax
	movw	%ax, -64(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	-64(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -64(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$3, %eax
	movw	%ax, -200(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$31, %eax
	shll	$1, %eax
	movw	%ax, -56(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	-56(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -56(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movw	%ax, -192(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movw	%ax, -190(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movw	%ax, -188(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movswl	-188(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -188(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movw	%ax, -186(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movw	%ax, -184(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$7, %eax
	movw	%ax, -182(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$2, %eax
	andl	$7, %eax
	movw	%ax, -180(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$3, %eax
	shll	$1, %eax
	movw	%ax, -178(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	-178(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -178(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movw	%ax, -176(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movw	%ax, -174(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movw	%ax, -172(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movswl	-172(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -172(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movw	%ax, -170(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movw	%ax, -168(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$127, %eax
	movw	%ax, -206(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$1, %eax
	movw	%ax, -62(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	-62(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -62(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$3, %eax
	movw	%ax, -198(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$31, %eax
	shll	$1, %eax
	movw	%ax, -54(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	-54(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -54(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movw	%ax, -166(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movw	%ax, -164(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movw	%ax, -162(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movswl	-162(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -162(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movw	%ax, -160(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movw	%ax, -158(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$7, %eax
	movw	%ax, -156(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$2, %eax
	andl	$7, %eax
	movw	%ax, -154(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$3, %eax
	shll	$1, %eax
	movw	%ax, -152(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	-152(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -152(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movw	%ax, -150(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movw	%ax, -148(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movw	%ax, -146(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movswl	-146(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -146(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movw	%ax, -144(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movw	%ax, -142(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$127, %eax
	movw	%ax, -204(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$1, %eax
	movw	%ax, -60(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	-60(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -60(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$3, %eax
	movw	%ax, -196(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$31, %eax
	shll	$1, %eax
	movw	%ax, -52(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	-52(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -52(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movw	%ax, -140(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movw	%ax, -138(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movw	%ax, -136(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movswl	-136(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -136(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movw	%ax, -134(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movw	%ax, -132(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$7, %eax
	movw	%ax, -130(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$2, %eax
	andl	$7, %eax
	movw	%ax, -128(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$3, %eax
	shll	$1, %eax
	movw	%ax, -126(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	-126(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -126(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movw	%ax, -124(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movw	%ax, -122(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movw	%ax, -120(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movswl	-120(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -120(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movw	%ax, -118(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movw	%ax, -116(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$127, %eax
	movw	%ax, -202(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$1, %eax
	movw	%ax, -58(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	-58(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -58(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$3, %eax
	movw	%ax, -194(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$31, %eax
	shll	$1, %eax
	movw	%ax, -50(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	-50(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -50(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movw	%ax, -114(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movw	%ax, -112(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movw	%ax, -110(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movswl	-110(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -110(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movw	%ax, -108(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movw	%ax, -106(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$7, %eax
	movw	%ax, -104(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$2, %eax
	andl	$7, %eax
	movw	%ax, -102(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$3, %eax
	shll	$1, %eax
	movw	%ax, -100(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	-100(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -100(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movw	%ax, -98(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movw	%ax, -96(%rbp)
	movq	-48(%rbp), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, -48(%rbp)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movw	%ax, -94(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movswl	-94(%rbp), %ecx
	orl	%eax, %ecx
	movw	%cx, -94(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movw	%ax, -92(%rbp)
	movq	-48(%rbp), %rax
	movzbl	(%rax), %eax
	andl	$7, %eax
	movw	%ax, -90(%rbp)
	movq	-88(%rbp), %rdi
	movswl	-80(%rbp), %edx
	movswl	-78(%rbp), %ecx
	movswl	-76(%rbp), %r8d
	movswl	-74(%rbp), %r9d
	movswl	-72(%rbp), %eax
	movswl	-70(%rbp), %r10d
	movswl	-68(%rbp), %r11d
	movswl	-66(%rbp), %ebx
	movabsq	$.str, %rsi
	movl	%eax, (%rsp)
	movl	%r10d, 8(%rsp)
	movl	%r11d, 16(%rsp)
	movl	%ebx, 24(%rsp)
	movb	$0, %al
	callq	fprintf
	movq	-88(%rbp), %rdi
	movswl	-208(%rbp), %edx
	movswl	-64(%rbp), %ecx
	movswl	-200(%rbp), %r8d
	movswl	-56(%rbp), %r9d
	movabsq	$.str.1, %rsi
	movb	$0, %al
	callq	fprintf
	movq	-88(%rbp), %rdi
	movswl	-192(%rbp), %edx
	movswl	-190(%rbp), %ecx
	movswl	-188(%rbp), %r8d
	movswl	-186(%rbp), %r9d
	movswl	-184(%rbp), %eax
	movswl	-182(%rbp), %ebx
	movswl	-180(%rbp), %r10d
	movswl	-178(%rbp), %r11d
	movswl	-176(%rbp), %r14d
	movswl	-174(%rbp), %r15d
	movswl	-172(%rbp), %r12d
	movswl	-170(%rbp), %r13d
	movswl	-168(%rbp), %esi
	movl	%esi, -216(%rbp)        # 4-byte Spill
	movabsq	$.str.2, %rsi
	movl	%eax, (%rsp)
	movl	%ebx, 8(%rsp)
	movl	%r10d, 16(%rsp)
	movl	%r11d, 24(%rsp)
	movl	%r14d, 32(%rsp)
	movl	%r15d, 40(%rsp)
	movl	%r12d, 48(%rsp)
	movl	%r13d, 56(%rsp)
	movl	-216(%rbp), %eax        # 4-byte Reload
	movl	%eax, 64(%rsp)
	movb	$0, %al
	callq	fprintf
	movq	-88(%rbp), %rdi
	movswl	-206(%rbp), %edx
	movswl	-62(%rbp), %ecx
	movswl	-198(%rbp), %r8d
	movswl	-54(%rbp), %r9d
	movabsq	$.str.3, %rsi
	movb	$0, %al
	callq	fprintf
	movq	-88(%rbp), %rdi
	movswl	-166(%rbp), %edx
	movswl	-164(%rbp), %ecx
	movswl	-162(%rbp), %r8d
	movswl	-160(%rbp), %r9d
	movswl	-158(%rbp), %eax
	movswl	-156(%rbp), %ebx
	movswl	-154(%rbp), %r10d
	movswl	-152(%rbp), %r11d
	movswl	-150(%rbp), %r14d
	movswl	-148(%rbp), %r15d
	movswl	-146(%rbp), %r12d
	movswl	-144(%rbp), %r13d
	movswl	-142(%rbp), %esi
	movl	%esi, -220(%rbp)        # 4-byte Spill
	movabsq	$.str.2, %rsi
	movl	%eax, (%rsp)
	movl	%ebx, 8(%rsp)
	movl	%r10d, 16(%rsp)
	movl	%r11d, 24(%rsp)
	movl	%r14d, 32(%rsp)
	movl	%r15d, 40(%rsp)
	movl	%r12d, 48(%rsp)
	movl	%r13d, 56(%rsp)
	movl	-220(%rbp), %eax        # 4-byte Reload
	movl	%eax, 64(%rsp)
	movb	$0, %al
	callq	fprintf
	movq	-88(%rbp), %rdi
	movswl	-204(%rbp), %edx
	movswl	-60(%rbp), %ecx
	movswl	-196(%rbp), %r8d
	movswl	-52(%rbp), %r9d
	movabsq	$.str.4, %rsi
	movb	$0, %al
	callq	fprintf
	movq	-88(%rbp), %rdi
	movswl	-140(%rbp), %edx
	movswl	-138(%rbp), %ecx
	movswl	-136(%rbp), %r8d
	movswl	-134(%rbp), %r9d
	movswl	-132(%rbp), %eax
	movswl	-130(%rbp), %ebx
	movswl	-128(%rbp), %r10d
	movswl	-126(%rbp), %r11d
	movswl	-124(%rbp), %r14d
	movswl	-122(%rbp), %r15d
	movswl	-120(%rbp), %r12d
	movswl	-118(%rbp), %r13d
	movswl	-116(%rbp), %esi
	movl	%esi, -224(%rbp)        # 4-byte Spill
	movabsq	$.str.2, %rsi
	movl	%eax, (%rsp)
	movl	%ebx, 8(%rsp)
	movl	%r10d, 16(%rsp)
	movl	%r11d, 24(%rsp)
	movl	%r14d, 32(%rsp)
	movl	%r15d, 40(%rsp)
	movl	%r12d, 48(%rsp)
	movl	%r13d, 56(%rsp)
	movl	-224(%rbp), %eax        # 4-byte Reload
	movl	%eax, 64(%rsp)
	movb	$0, %al
	callq	fprintf
	movq	-88(%rbp), %rdi
	movswl	-202(%rbp), %edx
	movswl	-58(%rbp), %ecx
	movswl	-194(%rbp), %r8d
	movswl	-50(%rbp), %r9d
	movabsq	$.str.5, %rsi
	movb	$0, %al
	callq	fprintf
	movq	-88(%rbp), %rdi
	movswl	-114(%rbp), %edx
	movswl	-112(%rbp), %ecx
	movswl	-110(%rbp), %r8d
	movswl	-108(%rbp), %r9d
	movswl	-106(%rbp), %eax
	movswl	-104(%rbp), %ebx
	movswl	-102(%rbp), %r10d
	movswl	-100(%rbp), %r11d
	movswl	-98(%rbp), %r14d
	movswl	-96(%rbp), %r15d
	movswl	-94(%rbp), %r12d
	movswl	-92(%rbp), %r13d
	movswl	-90(%rbp), %esi
	movl	%esi, -228(%rbp)        # 4-byte Spill
	movabsq	$.str.2, %rsi
	movl	%eax, (%rsp)
	movl	%ebx, 8(%rsp)
	movl	%r10d, 16(%rsp)
	movl	%r11d, 24(%rsp)
	movl	%r14d, 32(%rsp)
	movl	%r15d, 40(%rsp)
	movl	%r12d, 48(%rsp)
	movl	%r13d, 56(%rsp)
	movl	-228(%rbp), %eax        # 4-byte Reload
	movl	%eax, 64(%rsp)
	movb	$0, %al
	callq	fprintf
	movl	$0, -212(%rbp)
.LBB0_3:                                # %"3"
	movl	-212(%rbp), %eax
	addq	$280, %rsp              # imm = 0x118
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa %rsp, 8
	retq
.Lfunc_end0:
	.size	gsm_print, .Lfunc_end0-gsm_print
	.cfi_endproc
                                        # -- End function
	.hidden	.str
	.hidden	.str.1
	.hidden	.str.2
	.hidden	.str.3
	.hidden	.str.4
	.hidden	.str.5
	.hidden	__profc_gsm_print
	.ident	"clang version 10.0.0 "
	.section	".note.GNU-stack","",@progbits
