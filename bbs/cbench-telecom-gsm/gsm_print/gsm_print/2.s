	.text
	.file	"gsm_print.c"
	.globl	gsm_print.2             # -- Begin function gsm_print.2
	.p2align	4, 0x90
	.type	gsm_print.2,@function
gsm_print.2:                            # @gsm_print.2
	.cfi_startproc
# %bb.0:                                # %newFuncRoot
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset %rbp, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$168, %rsp
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	movq	%rcx, %r14
	movq	%rdx, %r11
	movq	24(%rbp), %rdx
	movq	16(%rbp), %r15
	jmp	.LBB0_2
.LBB0_1:                                # %"3.exitStub"
	addq	$168, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa %rsp, 8
	retq
.LBB0_2:                                # %"2"
	.cfi_def_cfa %rbp, 16
	movq	__profc_gsm_print+8, %rax
	addq	$1, %rax
	movq	%rax, __profc_gsm_print+8
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$15, %eax
	shll	$2, %eax
	movw	%ax, (%rsi)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movswl	(%rsi), %ecx
	orl	%eax, %ecx
	movw	%cx, (%rsi)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$63, %eax
	movw	%ax, 2(%rsi)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$31, %eax
	movw	%ax, 4(%rsi)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$7, %eax
	shll	$2, %eax
	movw	%ax, 6(%rsi)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movswl	6(%rsi), %ecx
	orl	%eax, %ecx
	movw	%cx, 6(%rsi)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$2, %eax
	andl	$15, %eax
	movw	%ax, 8(%rsi)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$3, %eax
	shll	$2, %eax
	movw	%ax, 10(%rsi)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movswl	10(%rsi), %ecx
	orl	%eax, %ecx
	movw	%cx, 10(%rsi)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movw	%ax, 12(%rsi)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movw	%ax, 14(%rsi)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$127, %eax
	movw	%ax, (%r11)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$1, %eax
	movw	%ax, (%r14)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	(%r14), %ecx
	orl	%eax, %ecx
	movw	%cx, (%r14)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$3, %eax
	movw	%ax, (%r8)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$31, %eax
	shll	$1, %eax
	movw	%ax, (%r9)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	(%r9), %ecx
	orl	%eax, %ecx
	movw	%cx, (%r9)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movw	%ax, (%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movw	%ax, 2(%r15)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movw	%ax, 4(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movswl	4(%r15), %ecx
	orl	%eax, %ecx
	movw	%cx, 4(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movw	%ax, 6(%r15)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movw	%ax, 8(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$7, %eax
	movw	%ax, 10(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$2, %eax
	andl	$7, %eax
	movw	%ax, 12(%r15)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$3, %eax
	shll	$1, %eax
	movw	%ax, 14(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	14(%r15), %ecx
	orl	%eax, %ecx
	movw	%cx, 14(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movw	%ax, 16(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movw	%ax, 18(%r15)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movw	%ax, 20(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movswl	20(%r15), %ecx
	orl	%eax, %ecx
	movw	%cx, 20(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movw	%ax, 22(%r15)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movw	%ax, 24(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$127, %eax
	movw	%ax, 2(%r11)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$1, %eax
	movw	%ax, 2(%r14)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	2(%r14), %ecx
	orl	%eax, %ecx
	movw	%cx, 2(%r14)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$3, %eax
	movw	%ax, 2(%r8)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$31, %eax
	shll	$1, %eax
	movw	%ax, 2(%r9)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	2(%r9), %ecx
	orl	%eax, %ecx
	movw	%cx, 2(%r9)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movw	%ax, 26(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movw	%ax, 28(%r15)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movw	%ax, 30(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movswl	30(%r15), %ecx
	orl	%eax, %ecx
	movw	%cx, 30(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movw	%ax, 32(%r15)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movw	%ax, 34(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$7, %eax
	movw	%ax, 36(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$2, %eax
	andl	$7, %eax
	movw	%ax, 38(%r15)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$3, %eax
	shll	$1, %eax
	movw	%ax, 40(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	40(%r15), %ecx
	orl	%eax, %ecx
	movw	%cx, 40(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movw	%ax, 42(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movw	%ax, 44(%r15)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movw	%ax, 46(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movswl	46(%r15), %ecx
	orl	%eax, %ecx
	movw	%cx, 46(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movw	%ax, 48(%r15)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movw	%ax, 50(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$127, %eax
	movw	%ax, 4(%r11)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$1, %eax
	movw	%ax, 4(%r14)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	4(%r14), %ecx
	orl	%eax, %ecx
	movw	%cx, 4(%r14)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$3, %eax
	movw	%ax, 4(%r8)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$31, %eax
	shll	$1, %eax
	movw	%ax, 4(%r9)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	4(%r9), %ecx
	orl	%eax, %ecx
	movw	%cx, 4(%r9)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movw	%ax, 52(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movw	%ax, 54(%r15)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movw	%ax, 56(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movswl	56(%r15), %ecx
	orl	%eax, %ecx
	movw	%cx, 56(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movw	%ax, 58(%r15)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movw	%ax, 60(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$7, %eax
	movw	%ax, 62(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$2, %eax
	andl	$7, %eax
	movw	%ax, 64(%r15)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$3, %eax
	shll	$1, %eax
	movw	%ax, 66(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	66(%r15), %ecx
	orl	%eax, %ecx
	movw	%cx, 66(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movw	%ax, 68(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movw	%ax, 70(%r15)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movw	%ax, 72(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movswl	72(%r15), %ecx
	orl	%eax, %ecx
	movw	%cx, 72(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movw	%ax, 74(%r15)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movw	%ax, 76(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$127, %eax
	movw	%ax, 6(%r11)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$1, %eax
	movw	%ax, 6(%r14)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	6(%r14), %ecx
	orl	%eax, %ecx
	movw	%cx, 6(%r14)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$3, %eax
	movw	%ax, 6(%r8)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$31, %eax
	shll	$1, %eax
	movw	%ax, 6(%r9)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	6(%r9), %ecx
	orl	%eax, %ecx
	movw	%cx, 6(%r9)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movw	%ax, 78(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movw	%ax, 80(%r15)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movw	%ax, 82(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movswl	82(%r15), %ecx
	orl	%eax, %ecx
	movw	%cx, 82(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movw	%ax, 84(%r15)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movw	%ax, 86(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$7, %eax
	movw	%ax, 88(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$2, %eax
	andl	$7, %eax
	movw	%ax, 90(%r15)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$3, %eax
	shll	$1, %eax
	movw	%ax, 92(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movswl	92(%r15), %ecx
	orl	%eax, %ecx
	movw	%cx, 92(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movw	%ax, 94(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movw	%ax, 96(%r15)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movw	%ax, 98(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movswl	98(%r15), %ecx
	orl	%eax, %ecx
	movw	%cx, 98(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movw	%ax, 100(%r15)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	andl	$7, %eax
	movw	%ax, 102(%r15)
	movq	(%rdx), %rdi
	movswl	(%rsi), %edx
	movswl	2(%rsi), %ecx
	movq	%r8, %r12
	movq	%r12, -120(%rbp)        # 8-byte Spill
	movswl	4(%rsi), %r8d
	movq	%r9, %r13
	movq	%r13, -128(%rbp)        # 8-byte Spill
	movswl	6(%rsi), %r9d
	movswl	8(%rsi), %eax
	movswl	10(%rsi), %ebx
	movl	%ebx, -44(%rbp)         # 4-byte Spill
	movq	%r14, -112(%rbp)        # 8-byte Spill
	movswl	12(%rsi), %r10d
	movq	%r11, %rbx
	movq	%rbx, -104(%rbp)        # 8-byte Spill
	movswl	14(%rsi), %r11d
	movabsq	$.str, %rsi
	movl	%eax, (%rsp)
	movl	-44(%rbp), %eax         # 4-byte Reload
	movl	%eax, 8(%rsp)
	movl	%r10d, 16(%rsp)
	movl	%r11d, 24(%rsp)
	movb	$0, %al
	callq	fprintf
	movq	24(%rbp), %rax
	movq	(%rax), %rdi
	movswl	(%rbx), %edx
	movswl	(%r14), %ecx
	movswl	(%r12), %r8d
	movswl	(%r13), %r9d
	movabsq	$.str.1, %rsi
	movb	$0, %al
	callq	fprintf
	movq	24(%rbp), %rax
	movq	(%rax), %rdi
	movswl	(%r15), %edx
	movswl	2(%r15), %ecx
	movswl	4(%r15), %r8d
	movswl	6(%r15), %r9d
	movswl	8(%r15), %eax
	movswl	10(%r15), %ebx
	movswl	12(%r15), %r10d
	movswl	14(%r15), %r11d
	movswl	16(%r15), %r12d
	movswl	18(%r15), %r14d
	movswl	20(%r15), %r13d
	movswl	22(%r15), %esi
	movl	%esi, -48(%rbp)         # 4-byte Spill
	movswl	24(%r15), %esi
	movl	%esi, -52(%rbp)         # 4-byte Spill
	movabsq	$.str.2, %rsi
	movl	%eax, (%rsp)
	movl	%ebx, 8(%rsp)
	movl	%r10d, 16(%rsp)
	movl	%r11d, 24(%rsp)
	movl	%r12d, 32(%rsp)
	movl	%r14d, 40(%rsp)
	movl	%r13d, 48(%rsp)
	movl	-48(%rbp), %eax         # 4-byte Reload
	movl	%eax, 56(%rsp)
	movl	-52(%rbp), %eax         # 4-byte Reload
	movl	%eax, 64(%rsp)
	movb	$0, %al
	callq	fprintf
	movq	24(%rbp), %rax
	movq	(%rax), %rdi
	movq	-104(%rbp), %r13        # 8-byte Reload
	movswl	2(%r13), %edx
	movq	-112(%rbp), %r12        # 8-byte Reload
	movswl	2(%r12), %ecx
	movq	-120(%rbp), %r14        # 8-byte Reload
	movswl	2(%r14), %r8d
	movq	-128(%rbp), %rbx        # 8-byte Reload
	movswl	2(%rbx), %r9d
	movabsq	$.str.3, %rsi
	movb	$0, %al
	callq	fprintf
	movq	24(%rbp), %rax
	movq	(%rax), %rdi
	movswl	26(%r15), %edx
	movswl	28(%r15), %ecx
	movswl	30(%r15), %r8d
	movswl	32(%r15), %r9d
	movswl	34(%r15), %eax
	movswl	36(%r15), %esi
	movl	%esi, -56(%rbp)         # 4-byte Spill
	movswl	38(%r15), %r10d
	movswl	40(%r15), %r11d
	movswl	42(%r15), %esi
	movl	%esi, -60(%rbp)         # 4-byte Spill
	movswl	44(%r15), %esi
	movl	%esi, -64(%rbp)         # 4-byte Spill
	movswl	46(%r15), %esi
	movl	%esi, -68(%rbp)         # 4-byte Spill
	movswl	48(%r15), %esi
	movl	%esi, -72(%rbp)         # 4-byte Spill
	movswl	50(%r15), %esi
	movl	%esi, -76(%rbp)         # 4-byte Spill
	movabsq	$.str.2, %rsi
	movl	%eax, (%rsp)
	movl	-56(%rbp), %eax         # 4-byte Reload
	movl	%eax, 8(%rsp)
	movl	%r10d, 16(%rsp)
	movl	%r11d, 24(%rsp)
	movl	-60(%rbp), %eax         # 4-byte Reload
	movl	%eax, 32(%rsp)
	movl	-64(%rbp), %eax         # 4-byte Reload
	movl	%eax, 40(%rsp)
	movl	-68(%rbp), %eax         # 4-byte Reload
	movl	%eax, 48(%rsp)
	movl	-72(%rbp), %eax         # 4-byte Reload
	movl	%eax, 56(%rsp)
	movl	-76(%rbp), %eax         # 4-byte Reload
	movl	%eax, 64(%rsp)
	movb	$0, %al
	callq	fprintf
	movq	24(%rbp), %rax
	movq	(%rax), %rdi
	movswl	4(%r13), %edx
	movswl	4(%r12), %ecx
	movswl	4(%r14), %r8d
	movswl	4(%rbx), %r9d
	movabsq	$.str.4, %rsi
	movb	$0, %al
	callq	fprintf
	movq	24(%rbp), %rax
	movq	(%rax), %rdi
	movswl	52(%r15), %edx
	movswl	54(%r15), %ecx
	movswl	56(%r15), %r8d
	movswl	58(%r15), %r9d
	movswl	60(%r15), %eax
	movswl	62(%r15), %ebx
	movswl	64(%r15), %r10d
	movswl	66(%r15), %r11d
	movswl	68(%r15), %r13d
	movswl	70(%r15), %r14d
	movswl	72(%r15), %r12d
	movswl	74(%r15), %esi
	movl	%esi, -80(%rbp)         # 4-byte Spill
	movswl	76(%r15), %esi
	movl	%esi, -84(%rbp)         # 4-byte Spill
	movabsq	$.str.2, %rsi
	movl	%eax, (%rsp)
	movl	%ebx, 8(%rsp)
	movl	%r10d, 16(%rsp)
	movl	%r11d, 24(%rsp)
	movl	%r13d, 32(%rsp)
	movl	%r14d, 40(%rsp)
	movl	%r12d, 48(%rsp)
	movl	-80(%rbp), %eax         # 4-byte Reload
	movl	%eax, 56(%rsp)
	movl	-84(%rbp), %eax         # 4-byte Reload
	movl	%eax, 64(%rsp)
	movb	$0, %al
	callq	fprintf
	movq	24(%rbp), %rbx
	movq	(%rbx), %rdi
	movq	-104(%rbp), %rax        # 8-byte Reload
	movswl	6(%rax), %edx
	movq	-112(%rbp), %rax        # 8-byte Reload
	movswl	6(%rax), %ecx
	movq	-120(%rbp), %rax        # 8-byte Reload
	movswl	6(%rax), %r8d
	movq	-128(%rbp), %rax        # 8-byte Reload
	movswl	6(%rax), %r9d
	movabsq	$.str.5, %rsi
	movb	$0, %al
	callq	fprintf
	movq	(%rbx), %rdi
	movswl	78(%r15), %edx
	movswl	80(%r15), %ecx
	movswl	82(%r15), %r8d
	movswl	84(%r15), %r9d
	movswl	86(%r15), %eax
	movswl	88(%r15), %ebx
	movswl	90(%r15), %r10d
	movswl	92(%r15), %r11d
	movswl	94(%r15), %esi
	movl	%esi, -88(%rbp)         # 4-byte Spill
	movswl	96(%r15), %r14d
	movswl	98(%r15), %r12d
	movswl	100(%r15), %esi
	movl	%esi, -92(%rbp)         # 4-byte Spill
	movswl	102(%r15), %r13d
	movabsq	$.str.2, %rsi
	movl	%eax, (%rsp)
	movl	%ebx, 8(%rsp)
	movl	%r10d, 16(%rsp)
	movl	%r11d, 24(%rsp)
	movl	-88(%rbp), %eax         # 4-byte Reload
	movl	%eax, 32(%rsp)
	movl	%r14d, 40(%rsp)
	movl	%r12d, 48(%rsp)
	movl	-92(%rbp), %eax         # 4-byte Reload
	movl	%eax, 56(%rsp)
	movl	%r13d, 64(%rsp)
	movb	$0, %al
	callq	fprintf
	movq	32(%rbp), %rax
	movl	$0, (%rax)
	jmp	.LBB0_1
.Lfunc_end0:
	.size	gsm_print.2, .Lfunc_end0-gsm_print.2
	.cfi_endproc
                                        # -- End function
	.hidden	.str
	.hidden	.str.1
	.hidden	.str.2
	.hidden	.str.3
	.hidden	.str.4
	.hidden	.str.5
	.hidden	__profc_gsm_print
	.ident	"clang version 10.0.0 "
	.section	".note.GNU-stack","",@progbits
