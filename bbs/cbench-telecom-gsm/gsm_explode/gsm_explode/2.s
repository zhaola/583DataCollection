	.text
	.file	"gsm_explode.c"
	.globl	gsm_explode.2           # -- Begin function gsm_explode.2
	.p2align	4, 0x90
	.type	gsm_explode.2,@function
gsm_explode.2:                          # @gsm_explode.2
	.cfi_startproc
# %bb.0:                                # %newFuncRoot
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset %rbp, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register %rbp
	jmp	.LBB0_2
.LBB0_1:                                # %"3.exitStub"
	popq	%rbp
	.cfi_def_cfa %rsp, 8
	retq
.LBB0_2:                                # %"2"
		movl $111, %ebx
		.byte 0x64, 0x67, 0x90
	.cfi_def_cfa %rbp, 16
	movq	__profc_gsm_explode+8, %rax
	addq	$1, %rax
	movq	%rax, __profc_gsm_explode+8
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$15, %eax
	shll	$2, %eax
	movq	(%rsi), %rcx
	movw	%ax, (%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movq	(%rsi), %r8
	movswl	(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, (%r8)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$63, %eax
	movq	(%rsi), %rcx
	movw	%ax, 2(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$31, %eax
	movq	(%rsi), %rcx
	movw	%ax, 4(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$7, %eax
	shll	$2, %eax
	movq	(%rsi), %rcx
	movw	%ax, 6(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movq	(%rsi), %r8
	movswl	6(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, 6(%r8)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$2, %eax
	andl	$15, %eax
	movq	(%rsi), %rcx
	movw	%ax, 8(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$3, %eax
	shll	$2, %eax
	movq	(%rsi), %rcx
	movw	%ax, 10(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movq	(%rsi), %r8
	movswl	10(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, 10(%r8)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 12(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 14(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$127, %eax
	movq	(%rsi), %rcx
	movw	%ax, 16(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$1, %eax
	movq	(%rsi), %rcx
	movw	%ax, 18(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	(%rsi), %r8
	movswl	18(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, 18(%r8)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$3, %eax
	movq	(%rsi), %rcx
	movw	%ax, 20(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$31, %eax
	shll	$1, %eax
	movq	(%rsi), %rcx
	movw	%ax, 22(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	(%rsi), %r8
	movswl	22(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, 22(%r8)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 24(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 26(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movq	(%rsi), %rcx
	movw	%ax, 28(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movq	(%rsi), %r8
	movswl	28(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, 28(%r8)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 30(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 32(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 34(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$2, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 36(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$3, %eax
	shll	$1, %eax
	movq	(%rsi), %rcx
	movw	%ax, 38(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	(%rsi), %r8
	movswl	38(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, 38(%r8)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 40(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 42(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movq	(%rsi), %rcx
	movw	%ax, 44(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movq	(%rsi), %r8
	movswl	44(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, 44(%r8)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 46(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 48(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$127, %eax
	movq	(%rsi), %rcx
	movw	%ax, 50(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$1, %eax
	movq	(%rsi), %rcx
	movw	%ax, 52(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	(%rsi), %r8
	movswl	52(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, 52(%r8)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$3, %eax
	movq	(%rsi), %rcx
	movw	%ax, 54(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$31, %eax
	shll	$1, %eax
	movq	(%rsi), %rcx
	movw	%ax, 56(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	(%rsi), %r8
	movswl	56(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, 56(%r8)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 58(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 60(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movq	(%rsi), %rcx
	movw	%ax, 62(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movq	(%rsi), %r8
	movswl	62(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, 62(%r8)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 64(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 66(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 68(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$2, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 70(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$3, %eax
	shll	$1, %eax
	movq	(%rsi), %rcx
	movw	%ax, 72(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	(%rsi), %r8
	movswl	72(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, 72(%r8)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 74(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 76(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movq	(%rsi), %rcx
	movw	%ax, 78(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movq	(%rsi), %r8
	movswl	78(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, 78(%r8)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 80(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 82(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$127, %eax
	movq	(%rsi), %rcx
	movw	%ax, 84(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$1, %eax
	movq	(%rsi), %rcx
	movw	%ax, 86(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	(%rsi), %r8
	movswl	86(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, 86(%r8)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$3, %eax
	movq	(%rsi), %rcx
	movw	%ax, 88(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$31, %eax
	shll	$1, %eax
	movq	(%rsi), %rcx
	movw	%ax, 90(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	(%rsi), %r8
	movswl	90(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, 90(%r8)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 92(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 94(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movq	(%rsi), %rcx
	movw	%ax, 96(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movq	(%rsi), %r8
	movswl	96(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, 96(%r8)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 98(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 100(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 102(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$2, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 104(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$3, %eax
	shll	$1, %eax
	movq	(%rsi), %rcx
	movw	%ax, 106(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	(%rsi), %r8
	movswl	106(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, 106(%r8)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 108(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 110(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movq	(%rsi), %rcx
	movw	%ax, 112(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movq	(%rsi), %r8
	movswl	112(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, 112(%r8)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 114(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 116(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$127, %eax
	movq	(%rsi), %rcx
	movw	%ax, 118(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$1, %eax
	movq	(%rsi), %rcx
	movw	%ax, 120(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	(%rsi), %r8
	movswl	120(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, 120(%r8)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$3, %eax
	movq	(%rsi), %rcx
	movw	%ax, 122(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$31, %eax
	shll	$1, %eax
	movq	(%rsi), %rcx
	movw	%ax, 124(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	(%rsi), %r8
	movswl	124(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, 124(%r8)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 126(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 128(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movq	(%rsi), %rcx
	movw	%ax, 130(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movq	(%rsi), %r8
	movswl	130(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, 130(%r8)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 132(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 134(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$5, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 136(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$2, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 138(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$3, %eax
	shll	$1, %eax
	movq	(%rsi), %rcx
	movw	%ax, 140(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$7, %eax
	andl	$1, %eax
	movq	(%rsi), %r8
	movswl	140(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, 140(%r8)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$4, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 142(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$1, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 144(%rcx)
	movq	(%rdi), %rax
	movq	%rax, %rcx
	addq	$1, %rcx
	movq	%rcx, (%rdi)
	movzbl	(%rax), %eax
	andl	$1, %eax
	shll	$2, %eax
	movq	(%rsi), %rcx
	movw	%ax, 146(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$6, %eax
	andl	$3, %eax
	movq	(%rsi), %r8
	movswl	146(%r8), %ecx
	orl	%eax, %ecx
	movw	%cx, 146(%r8)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	sarl	$3, %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 148(%rcx)
	movq	(%rdi), %rax
	movzbl	(%rax), %eax
	andl	$7, %eax
	movq	(%rsi), %rcx
	movw	%ax, 150(%rcx)
	movl	$0, (%rdx)
		movl $222, %ebx
		.byte 0x64, 0x67, 0x90
.Lfunc_end0:
	.size	gsm_explode.2, .Lfunc_end0-gsm_explode.2
	.cfi_endproc
                                        # -- End function
	.hidden	__profc_gsm_explode
	.ident	"clang version 10.0.0 "
	.section	".note.GNU-stack","",@progbits
