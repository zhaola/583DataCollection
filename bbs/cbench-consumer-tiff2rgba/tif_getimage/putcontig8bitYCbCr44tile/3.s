	.text
	.file	"tif_getimage.c"
	.globl	putcontig8bitYCbCr44tile.3 # -- Begin function putcontig8bitYCbCr44tile.3
	.p2align	4, 0x90
	.type	putcontig8bitYCbCr44tile.3,@function
putcontig8bitYCbCr44tile.3:             # @putcontig8bitYCbCr44tile.3
	.cfi_startproc
# %bb.0:                                # %newFuncRoot
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset %rbp, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	movq	%rdx, %r15
	movq	24(%rbp), %r12
	movq	16(%rbp), %rdx
	jmp	.LBB0_2
.LBB0_1:                                # %"4.exitStub"
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa %rsp, 8
	retq
.LBB0_2:                                # %"3"
		movl $111, %ebx
		.byte 0x64, 0x67, 0x90
	.cfi_def_cfa %rbp, 16
	movq	__profc_.._tif_getimage.c_putcontig8bitYCbCr44tile, %rbx
	addq	$1, %rbx
	movq	%rbx, __profc_.._tif_getimage.c_putcontig8bitYCbCr44tile
	movq	(%rdi), %rbx
	movzbl	16(%rbx), %ebx
	movl	%ebx, (%rsi)
	movq	(%rdi), %rbx
	movzbl	17(%rbx), %ebx
	movl	%ebx, (%r15)
	movq	(%rdi), %rbx
	movzbl	(%rbx), %ebx
	movl	%ebx, (%rcx)
	movq	(%r8), %r14
	movl	(%rcx), %eax
	movq	(%r9), %rbx
	movslq	(%r15), %r10
	addl	(%rbx,%r10,4), %eax
	cltq
	movzbl	(%r14,%rax), %r14d
	movq	(%r8), %r10
	movl	(%rcx), %ebx
	movq	(%rdx), %rax
	movslq	(%rsi), %r13
	movl	(%rax,%r13,4), %eax
	movq	(%r12), %r13
	movslq	(%r15), %r11
	addl	(%r13,%r11,4), %eax
	sarl	$16, %eax
	addl	%eax, %ebx
	movslq	%ebx, %rax
	movzbl	(%r10,%rax), %eax
	shll	$8, %eax
	orl	%eax, %r14d
	movq	(%r8), %r10
	movl	(%rcx), %ecx
	movq	32(%rbp), %rax
	movq	(%rax), %rbx
	movslq	(%rsi), %rax
	addl	(%rbx,%rax,4), %ecx
	movslq	%ecx, %rax
	movzbl	(%r10,%rax), %eax
	shll	$16, %eax
	orl	%eax, %r14d
	orl	$-16777216, %r14d       # imm = 0xFF000000
	movq	40(%rbp), %rax
	movq	(%rax), %rax
	movl	%r14d, (%rax)
	movq	(%rdi), %rax
	movzbl	1(%rax), %eax
	movq	48(%rbp), %rdx
	movl	%eax, (%rdx)
	movq	(%r8), %r10
	movl	(%rdx), %ecx
	movq	(%r9), %rbx
	movslq	(%r15), %rax
	addl	(%rbx,%rax,4), %ecx
	movslq	%ecx, %rax
	movzbl	(%r10,%rax), %ecx
	movq	(%r8), %r10
	movl	(%rdx), %r14d
	movq	16(%rbp), %r13
	movq	(%r13), %r11
	movslq	(%rsi), %rax
	movl	(%r11,%rax,4), %eax
	movq	(%r12), %r11
	movslq	(%r15), %rbx
	addl	(%r11,%rbx,4), %eax
	sarl	$16, %eax
	addl	%eax, %r14d
	movslq	%r14d, %rax
	movzbl	(%r10,%rax), %eax
	shll	$8, %eax
	orl	%eax, %ecx
	movq	(%r8), %r10
	movl	(%rdx), %ebx
	movq	32(%rbp), %rax
	movq	(%rax), %r11
	movslq	(%rsi), %rax
	addl	(%r11,%rax,4), %ebx
	movslq	%ebx, %rax
	movzbl	(%r10,%rax), %eax
	shll	$16, %eax
	orl	%eax, %ecx
	orl	$-16777216, %ecx        # imm = 0xFF000000
	movq	40(%rbp), %rax
	movq	(%rax), %rax
	movl	%ecx, 4(%rax)
	movq	(%rdi), %rax
	movzbl	2(%rax), %eax
	movq	56(%rbp), %rdx
	movl	%eax, (%rdx)
	movq	(%r8), %r10
	movl	(%rdx), %ecx
	movq	(%r9), %rbx
	movq	%r9, %r12
	movslq	(%r15), %rax
	addl	(%rbx,%rax,4), %ecx
	movslq	%ecx, %rax
	movzbl	(%r10,%rax), %ecx
	movq	(%r8), %r10
	movl	(%rdx), %r14d
	movq	(%r13), %r11
	movslq	(%rsi), %rax
	movl	(%r11,%rax,4), %eax
	movq	24(%rbp), %rbx
	movq	(%rbx), %r11
	movslq	(%r15), %rbx
	addl	(%r11,%rbx,4), %eax
	sarl	$16, %eax
	addl	%eax, %r14d
	movslq	%r14d, %rax
	movzbl	(%r10,%rax), %eax
	shll	$8, %eax
	orl	%eax, %ecx
	movq	(%r8), %r10
	movl	(%rdx), %ebx
	movq	32(%rbp), %r13
	movq	(%r13), %r11
	movslq	(%rsi), %rax
	addl	(%r11,%rax,4), %ebx
	movslq	%ebx, %rax
	movzbl	(%r10,%rax), %eax
	shll	$16, %eax
	orl	%eax, %ecx
	orl	$-16777216, %ecx        # imm = 0xFF000000
	movq	40(%rbp), %rax
	movq	(%rax), %rax
	movl	%ecx, 8(%rax)
	movq	(%rdi), %rax
	movq	%rdi, %r9
	movzbl	3(%rax), %eax
	movq	64(%rbp), %rdi
	movq	%r15, %rdx
	movl	%eax, (%rdi)
	movq	(%r8), %r10
	movl	(%rdi), %ecx
	movq	%r12, %r15
	movq	(%r15), %rbx
	movslq	(%rdx), %rax
	addl	(%rbx,%rax,4), %ecx
	movslq	%ecx, %rax
	movzbl	(%r10,%rax), %ecx
	movq	(%r8), %r10
	movl	(%rdi), %r14d
	movq	16(%rbp), %rax
	movq	(%rax), %r11
	movslq	(%rsi), %rax
	movl	(%r11,%rax,4), %eax
	movq	24(%rbp), %rbx
	movq	(%rbx), %r11
	movslq	(%rdx), %rbx
	addl	(%r11,%rbx,4), %eax
	sarl	$16, %eax
	addl	%eax, %r14d
	movslq	%r14d, %rax
	movzbl	(%r10,%rax), %eax
	shll	$8, %eax
	orl	%eax, %ecx
	movq	(%r8), %r10
	movl	(%rdi), %ebx
	movq	(%r13), %r11
	movslq	(%rsi), %rax
	addl	(%r11,%rax,4), %ebx
	movslq	%ebx, %rax
	movzbl	(%r10,%rax), %eax
	shll	$16, %eax
	orl	%eax, %ecx
	orl	$-16777216, %ecx        # imm = 0xFF000000
	movq	40(%rbp), %rax
	movq	(%rax), %rax
	movl	%ecx, 12(%rax)
	movq	(%r9), %rax
	movq	%r9, %r12
	movzbl	4(%rax), %eax
	movq	72(%rbp), %rcx
	movq	%rcx, %r9
	movl	%eax, (%r9)
	movq	(%r8), %r10
	movl	(%r9), %ecx
	movq	(%r15), %rbx
	movslq	(%rdx), %rax
	addl	(%rbx,%rax,4), %ecx
	movslq	%ecx, %rax
	movzbl	(%r10,%rax), %ecx
	movq	(%r8), %r10
	movl	(%r9), %r14d
	movq	16(%rbp), %rax
	movq	(%rax), %r11
	movslq	(%rsi), %rax
	movl	(%r11,%rax,4), %eax
	movq	24(%rbp), %rbx
	movq	(%rbx), %r11
	movslq	(%rdx), %rbx
	addl	(%r11,%rbx,4), %eax
	sarl	$16, %eax
	addl	%eax, %r14d
	movslq	%r14d, %rax
	movzbl	(%r10,%rax), %eax
	shll	$8, %eax
	orl	%eax, %ecx
	movq	(%r8), %r10
	movl	(%r9), %ebx
	movq	(%r13), %r11
	movslq	(%rsi), %rax
	addl	(%r11,%rax,4), %ebx
	movslq	%ebx, %rax
	movzbl	(%r10,%rax), %eax
	shll	$16, %eax
	orl	%eax, %ecx
	orl	$-16777216, %ecx        # imm = 0xFF000000
	movq	80(%rbp), %r13
	movq	(%r13), %rax
	movl	%ecx, (%rax)
	movq	(%r12), %rax
	movzbl	5(%rax), %eax
	movq	88(%rbp), %rcx
	movq	%rcx, %r9
	movl	%eax, (%r9)
	movq	(%r8), %r10
	movl	(%r9), %ecx
	movq	%r15, %rdi
	movq	(%rdi), %rbx
	movslq	(%rdx), %rax
	addl	(%rbx,%rax,4), %ecx
	movslq	%ecx, %rax
	movzbl	(%r10,%rax), %ecx
	movq	(%r8), %r10
	movl	(%r9), %r14d
	movq	16(%rbp), %rax
	movq	(%rax), %r11
	movslq	(%rsi), %rax
	movl	(%r11,%rax,4), %eax
	movq	24(%rbp), %r13
	movq	(%r13), %r11
	movslq	(%rdx), %rbx
	addl	(%r11,%rbx,4), %eax
	sarl	$16, %eax
	addl	%eax, %r14d
	movslq	%r14d, %rax
	movzbl	(%r10,%rax), %eax
	shll	$8, %eax
	orl	%eax, %ecx
	movq	(%r8), %r10
	movl	(%r9), %ebx
	movq	32(%rbp), %rax
	movq	(%rax), %r11
	movslq	(%rsi), %rax
	addl	(%r11,%rax,4), %ebx
	movslq	%ebx, %rax
	movzbl	(%r10,%rax), %eax
	shll	$16, %eax
	orl	%eax, %ecx
	orl	$-16777216, %ecx        # imm = 0xFF000000
	movq	80(%rbp), %rax
	movq	(%rax), %rax
	movl	%ecx, 4(%rax)
	movq	(%r12), %rax
	movzbl	6(%rax), %eax
	movq	96(%rbp), %r9
	movl	%eax, (%r9)
	movq	(%r8), %r10
	movl	(%r9), %ecx
	movq	(%rdi), %rbx
	movslq	(%rdx), %rax
	addl	(%rbx,%rax,4), %ecx
	movslq	%ecx, %rax
	movzbl	(%r10,%rax), %ecx
	movq	(%r8), %r10
	movl	(%r9), %r14d
	movq	16(%rbp), %rax
	movq	(%rax), %r11
	movslq	(%rsi), %rax
	movl	(%r11,%rax,4), %eax
	movq	(%r13), %r11
	movslq	(%rdx), %rbx
	addl	(%r11,%rbx,4), %eax
	sarl	$16, %eax
	addl	%eax, %r14d
	movslq	%r14d, %rax
	movzbl	(%r10,%rax), %eax
	shll	$8, %eax
	orl	%eax, %ecx
	movq	(%r8), %r10
	movl	(%r9), %ebx
	movq	32(%rbp), %rax
	movq	(%rax), %r11
	movslq	(%rsi), %rax
	addl	(%r11,%rax,4), %ebx
	movslq	%ebx, %rax
	movzbl	(%r10,%rax), %eax
	shll	$16, %eax
	orl	%eax, %ecx
	orl	$-16777216, %ecx        # imm = 0xFF000000
	movq	80(%rbp), %rax
	movq	(%rax), %rax
	movl	%ecx, 8(%rax)
	movq	%r12, %r15
	movq	(%r15), %rax
	movzbl	7(%rax), %eax
	movq	104(%rbp), %rcx
	movq	%rcx, %r12
	movl	%eax, (%r12)
	movq	(%r8), %r10
	movl	(%r12), %ecx
	movq	%rdi, %r9
	movq	(%r9), %rbx
	movslq	(%rdx), %rax
	addl	(%rbx,%rax,4), %ecx
	movslq	%ecx, %rax
	movzbl	(%r10,%rax), %ecx
	movq	(%r8), %r10
	movl	(%r12), %r14d
	movq	16(%rbp), %rax
	movq	(%rax), %r11
	movslq	(%rsi), %rax
	movl	(%r11,%rax,4), %eax
	movq	(%r13), %r11
	movslq	(%rdx), %rbx
	addl	(%r11,%rbx,4), %eax
	sarl	$16, %eax
	addl	%eax, %r14d
	movslq	%r14d, %rax
	movzbl	(%r10,%rax), %eax
	shll	$8, %eax
	orl	%eax, %ecx
	movq	(%r8), %r10
	movl	(%r12), %ebx
	movq	32(%rbp), %r12
	movq	(%r12), %r11
	movslq	(%rsi), %rax
	addl	(%r11,%rax,4), %ebx
	movslq	%ebx, %rax
	movzbl	(%r10,%rax), %eax
	shll	$16, %eax
	orl	%eax, %ecx
	orl	$-16777216, %ecx        # imm = 0xFF000000
	movq	80(%rbp), %rax
	movq	(%rax), %rax
	movl	%ecx, 12(%rax)
	movq	(%r15), %rax
	movq	%r15, %rdi
	movzbl	8(%rax), %eax
	movq	112(%rbp), %rcx
	movq	%rcx, %r15
	movl	%eax, (%r15)
	movq	(%r8), %r10
	movl	(%r15), %ecx
	movq	(%r9), %rbx
	movslq	(%rdx), %rax
	addl	(%rbx,%rax,4), %ecx
	movslq	%ecx, %rax
	movzbl	(%r10,%rax), %ecx
	movq	(%r8), %r10
	movl	(%r15), %r14d
	movq	16(%rbp), %rax
	movq	(%rax), %r11
	movslq	(%rsi), %rax
	movl	(%r11,%rax,4), %eax
	movq	(%r13), %r11
	movslq	(%rdx), %rbx
	addl	(%r11,%rbx,4), %eax
	sarl	$16, %eax
	addl	%eax, %r14d
	movslq	%r14d, %rax
	movzbl	(%r10,%rax), %eax
	shll	$8, %eax
	orl	%eax, %ecx
	movq	(%r8), %r10
	movl	(%r15), %ebx
	movq	(%r12), %r11
	movslq	(%rsi), %rax
	addl	(%r11,%rax,4), %ebx
	movslq	%ebx, %rax
	movzbl	(%r10,%rax), %eax
	shll	$16, %eax
	orl	%eax, %ecx
	orl	$-16777216, %ecx        # imm = 0xFF000000
	movq	120(%rbp), %r12
	movq	(%r12), %rax
	movl	%ecx, (%rax)
	movq	(%rdi), %rax
	movzbl	9(%rax), %eax
	movq	128(%rbp), %rcx
	movq	%rcx, %r15
	movl	%eax, (%r15)
	movq	(%r8), %r10
	movl	(%r15), %ecx
	movq	(%r9), %rbx
	movq	%rdx, %r13
	movslq	(%r13), %rax
	addl	(%rbx,%rax,4), %ecx
	movslq	%ecx, %rax
	movzbl	(%r10,%rax), %ecx
	movq	(%r8), %r10
	movl	(%r15), %r14d
	movq	16(%rbp), %rax
	movq	(%rax), %r11
	movq	%rsi, %rdx
	movslq	(%rdx), %rax
	movl	(%r11,%rax,4), %eax
	movq	24(%rbp), %rsi
	movq	(%rsi), %r11
	movslq	(%r13), %rbx
	addl	(%r11,%rbx,4), %eax
	sarl	$16, %eax
	addl	%eax, %r14d
	movslq	%r14d, %rax
	movzbl	(%r10,%rax), %eax
	shll	$8, %eax
	orl	%eax, %ecx
	movq	(%r8), %r10
	movl	(%r15), %ebx
	movq	32(%rbp), %r15
	movq	(%r15), %r11
	movslq	(%rdx), %rax
	movq	%rdx, %rsi
	addl	(%r11,%rax,4), %ebx
	movslq	%ebx, %rax
	movzbl	(%r10,%rax), %eax
	shll	$16, %eax
	orl	%eax, %ecx
	orl	$-16777216, %ecx        # imm = 0xFF000000
	movq	(%r12), %rax
	movl	%ecx, 4(%rax)
	movq	(%rdi), %rax
	movq	%rdi, %rdx
	movzbl	10(%rax), %eax
	movq	136(%rbp), %rcx
	movq	%rcx, %r12
	movl	%eax, (%r12)
	movq	(%r8), %r10
	movl	(%r12), %ecx
	movq	%r9, %rdi
	movq	(%rdi), %rbx
	movslq	(%r13), %rax
	addl	(%rbx,%rax,4), %ecx
	movslq	%ecx, %rax
	movzbl	(%r10,%rax), %ecx
	movq	(%r8), %r10
	movl	(%r12), %r14d
	movq	16(%rbp), %r9
	movq	(%r9), %r11
	movslq	(%rsi), %rax
	movl	(%r11,%rax,4), %eax
	movq	24(%rbp), %rbx
	movq	(%rbx), %r11
	movslq	(%r13), %rbx
	addl	(%r11,%rbx,4), %eax
	sarl	$16, %eax
	addl	%eax, %r14d
	movslq	%r14d, %rax
	movzbl	(%r10,%rax), %eax
	shll	$8, %eax
	orl	%eax, %ecx
	movq	(%r8), %r10
	movl	(%r12), %ebx
	movq	(%r15), %r11
	movslq	(%rsi), %rax
	addl	(%r11,%rax,4), %ebx
	movslq	%ebx, %rax
	movzbl	(%r10,%rax), %eax
	shll	$16, %eax
	orl	%eax, %ecx
	orl	$-16777216, %ecx        # imm = 0xFF000000
	movq	120(%rbp), %r12
	movq	(%r12), %rax
	movl	%ecx, 8(%rax)
	movq	(%rdx), %rax
	movzbl	11(%rax), %eax
	movq	144(%rbp), %rcx
	movq	%rcx, %r15
	movl	%eax, (%r15)
	movq	(%r8), %r10
	movl	(%r15), %ecx
	movq	(%rdi), %rbx
	movslq	(%r13), %rax
	addl	(%rbx,%rax,4), %ecx
	movslq	%ecx, %rax
	movzbl	(%r10,%rax), %ecx
	movq	(%r8), %r10
	movl	(%r15), %r14d
	movq	(%r9), %r11
	movslq	(%rsi), %rax
	movl	(%r11,%rax,4), %eax
	movq	24(%rbp), %r9
	movq	(%r9), %r11
	movslq	(%r13), %rbx
	addl	(%r11,%rbx,4), %eax
	sarl	$16, %eax
	addl	%eax, %r14d
	movslq	%r14d, %rax
	movzbl	(%r10,%rax), %eax
	shll	$8, %eax
	orl	%eax, %ecx
	movq	(%r8), %r10
	movl	(%r15), %ebx
	movq	32(%rbp), %rax
	movq	(%rax), %r11
	movslq	(%rsi), %rax
	addl	(%r11,%rax,4), %ebx
	movslq	%ebx, %rax
	movzbl	(%r10,%rax), %eax
	shll	$16, %eax
	orl	%eax, %ecx
	orl	$-16777216, %ecx        # imm = 0xFF000000
	movq	(%r12), %rax
	movl	%ecx, 12(%rax)
	movq	(%rdx), %rax
	movzbl	12(%rax), %eax
	movq	152(%rbp), %rcx
	movq	%rcx, %r15
	movl	%eax, (%r15)
	movq	(%r8), %r10
	movl	(%r15), %ecx
	movq	(%rdi), %rbx
	movslq	(%r13), %rax
	addl	(%rbx,%rax,4), %ecx
	movslq	%ecx, %rax
	movzbl	(%r10,%rax), %ecx
	movq	(%r8), %r10
	movl	(%r15), %r14d
	movq	16(%rbp), %r12
	movq	(%r12), %r11
	movslq	(%rsi), %rax
	movl	(%r11,%rax,4), %eax
	movq	(%r9), %r11
	movslq	(%r13), %rbx
	addl	(%r11,%rbx,4), %eax
	sarl	$16, %eax
	addl	%eax, %r14d
	movslq	%r14d, %rax
	movzbl	(%r10,%rax), %eax
	shll	$8, %eax
	orl	%eax, %ecx
	movq	(%r8), %r10
	movl	(%r15), %ebx
	movq	32(%rbp), %rax
	movq	(%rax), %r11
	movslq	(%rsi), %rax
	addl	(%r11,%rax,4), %ebx
	movslq	%ebx, %rax
	movzbl	(%r10,%rax), %eax
	shll	$16, %eax
	orl	%eax, %ecx
	orl	$-16777216, %ecx        # imm = 0xFF000000
	movq	160(%rbp), %r15
	movq	(%r15), %rax
	movl	%ecx, (%rax)
	movq	(%rdx), %rax
	movzbl	13(%rax), %eax
	movq	168(%rbp), %rcx
	movq	%rcx, %r15
	movl	%eax, (%r15)
	movq	(%r8), %r10
	movl	(%r15), %ecx
	movq	(%rdi), %rbx
	movslq	(%r13), %rax
	addl	(%rbx,%rax,4), %ecx
	movslq	%ecx, %rax
	movzbl	(%r10,%rax), %ecx
	movq	(%r8), %r10
	movl	(%r15), %r14d
	movq	(%r12), %r11
	movslq	(%rsi), %rax
	movl	(%r11,%rax,4), %eax
	movq	(%r9), %r11
	movq	%r9, %r12
	movslq	(%r13), %rbx
	addl	(%r11,%rbx,4), %eax
	sarl	$16, %eax
	addl	%eax, %r14d
	movslq	%r14d, %rax
	movzbl	(%r10,%rax), %eax
	shll	$8, %eax
	orl	%eax, %ecx
	movq	(%r8), %r10
	movl	(%r15), %ebx
	movq	32(%rbp), %rax
	movq	(%rax), %r11
	movslq	(%rsi), %rax
	addl	(%r11,%rax,4), %ebx
	movslq	%ebx, %rax
	movzbl	(%r10,%rax), %eax
	shll	$16, %eax
	orl	%eax, %ecx
	orl	$-16777216, %ecx        # imm = 0xFF000000
	movq	160(%rbp), %rax
	movq	(%rax), %rax
	movl	%ecx, 4(%rax)
	movq	(%rdx), %rax
	movzbl	14(%rax), %eax
	movq	176(%rbp), %r9
	movq	%rdx, %r15
	movl	%eax, (%r9)
	movq	(%r8), %r10
	movl	(%r9), %ecx
	movq	(%rdi), %rbx
	movslq	(%r13), %rax
	addl	(%rbx,%rax,4), %ecx
	movslq	%ecx, %rax
	movzbl	(%r10,%rax), %ecx
	movq	(%r8), %r10
	movl	(%r9), %r14d
	movq	16(%rbp), %rax
	movq	(%rax), %r11
	movslq	(%rsi), %rax
	movl	(%r11,%rax,4), %eax
	movq	(%r12), %r11
	movslq	(%r13), %rbx
	addl	(%r11,%rbx,4), %eax
	sarl	$16, %eax
	addl	%eax, %r14d
	movslq	%r14d, %rax
	movzbl	(%r10,%rax), %eax
	shll	$8, %eax
	orl	%eax, %ecx
	movq	(%r8), %r10
	movl	(%r9), %ebx
	movq	32(%rbp), %r14
	movq	(%r14), %r11
	movslq	(%rsi), %rax
	addl	(%r11,%rax,4), %ebx
	movslq	%ebx, %rax
	movzbl	(%r10,%rax), %eax
	shll	$16, %eax
	orl	%eax, %ecx
	orl	$-16777216, %ecx        # imm = 0xFF000000
	movq	160(%rbp), %rax
	movq	(%rax), %rax
	movl	%ecx, 8(%rax)
	movq	(%r15), %rax
	movzbl	15(%rax), %eax
	movq	184(%rbp), %rcx
	movq	%rcx, %r11
	movl	%eax, (%r11)
	movq	(%r8), %r10
	movl	(%r11), %ecx
	movq	(%rdi), %rbx
	movslq	(%r13), %rax
	addl	(%rbx,%rax,4), %ecx
	movslq	%ecx, %rax
	movzbl	(%r10,%rax), %ecx
	movq	(%r8), %r9
	movl	(%r11), %ebx
	movq	16(%rbp), %rax
	movq	(%rax), %r10
	movslq	(%rsi), %rax
	movl	(%r10,%rax,4), %eax
	movq	(%r12), %r10
	movslq	(%r13), %rdx
	addl	(%r10,%rdx,4), %eax
	sarl	$16, %eax
	addl	%eax, %ebx
	movslq	%ebx, %rax
	movzbl	(%r9,%rax), %eax
	shll	$8, %eax
	orl	%eax, %ecx
	movq	(%r8), %rax
	movl	(%r11), %edx
	movq	(%r14), %rbx
	movslq	(%rsi), %rsi
	addl	(%rbx,%rsi,4), %edx
	movslq	%edx, %rdx
	movzbl	(%rax,%rdx), %eax
	shll	$16, %eax
	orl	%eax, %ecx
	orl	$-16777216, %ecx        # imm = 0xFF000000
	movq	160(%rbp), %rdx
	movq	(%rdx), %rax
	movl	%ecx, 12(%rax)
	movq	40(%rbp), %rcx
	movq	(%rcx), %rax
	addq	$16, %rax
	movq	%rax, (%rcx)
	movq	80(%rbp), %rcx
	movq	(%rcx), %rax
	addq	$16, %rax
	movq	%rax, (%rcx)
	movq	120(%rbp), %rcx
	movq	(%rcx), %rax
	addq	$16, %rax
	movq	%rax, (%rcx)
	movq	%rdx, %rcx
	movq	(%rcx), %rax
	addq	$16, %rax
	movq	%rax, (%rcx)
	movq	(%r15), %rax
	addq	$18, %rax
	movq	%rax, (%r15)
		movl $222, %ebx
		.byte 0x64, 0x67, 0x90
.Lfunc_end0:
	.size	putcontig8bitYCbCr44tile.3, .Lfunc_end0-putcontig8bitYCbCr44tile.3
	.cfi_endproc
                                        # -- End function
	.hidden	__profc_.._tif_getimage.c_putcontig8bitYCbCr44tile
	.ident	"clang version 10.0.0 "
	.section	".note.GNU-stack","",@progbits
